Лайфаки в ML

Понимание задачи - Вникнуть в задачу, проанализировать спич презентации
Preprocessing - Применять не только технические инструенты, но и логику
Исследование данных - Чекнуть распределения и взаимосвязи
Выбор модели
Обучение и тестирование модели*
Оценка модели - От экспертов узнать о метрике
Fine-tuning - Ансамблирование моделей
Визуализация данных
Документация
Командная работа




 Подготовка данных:
        Загрузка данных из источников данных.
        Предварительная обработка данных, включая чистку, преобразование и обработку пропущенных значений.
        Разделение данных на тренировочный, валидационный и тестовый наборы.

    Инженерия признаков:
        Создание новых признаков для улучшения предсказательной способности модели.
        Применение методов масштабирования, кодирования категориальных признаков и нормализации данных.

    Выбор и обучение модели:
        Выбор модели в зависимости от задачи (например, классификация, регрессия).
        Обучение модели на тренировочном наборе данных.
        Настройка гиперпараметров для повышения производительности модели.

    Оценка модели:
        Оценка модели на валидационном наборе данных с использованием метрик, соответствующих конкретной задаче (например, accuracy, precision, recall).
        Анализ ошибок и обратная связь для улучшения модели.

    Тестирование модели:
        Оценка производительности модели на тестовом наборе данных, который модель не видела в процессе обучения и валидации.

    Внедрение модели:
        Создание интерфейса или API для интеграции модели с другими системами.
        Мониторинг производительности модели в производственной среде.
        Обновление модели при необходимости.

    Документация:
        Создание документации, описывающей шаги пайплайна, предположения, использованные методы и ограничения модели.

    Автоматизация:
        Использование средств автоматизации для выполнения шагов пайплайна.
        Настройка автоматического обучения модели при появлении новых данных.


# Пример обработки пропущенных значений
df['Возраст'] = df['Возраст'].fillna(df['Возраст'].mean())

# Пример создания нового признака
df['Статус'] = np.where(df['Возраст'] >= 30, 'Старше 30', 'Младше 30')

# Пример обработки категориальных признаков
df = pd.get_dummies(df, columns=['Пол'])

# Пример создания временного признака
df['Год рождения'] = pd.to_datetime('now').year - df['Возраст']

# Пример обработки выбросов
df['Цена'] = np.where(df['Цена'] > 1000, 1000, df['Цена'])

# Пример отбора признаков
selected_features = ['Имя', 'Возраст', 'Пол_Ж', 'Пол_М', 'Статус', 'Год рождения']
df = df[selected_features]

# Пример сохранения обработанных данных
df.to_csv('обработанные_данные.csv', index=False)


Инженерия признаков (feature engineering) важная часть процесса подготовки данных для машинного обучения.
Ниже приведены некоторые основные методы инженерии признаков с использованием библиотеки pandas в Python:

### 1. Создание новых признаков:

#### Пример: Извлечение длины строки из столбца "Текст"

```python
import pandas as pd

# Создание DataFrame
data = {'Текст': ['Привет', 'Мир', 'Как дела']}
df = pd.DataFrame(data)

# Создание нового признака "Длина_текста"
df['Длина_текста'] = df['Текст'].apply(len)

print(df)
```

### 2. Масштабирование признаков:

#### Пример: Стандартизация числовых признаков

```python
from sklearn.preprocessing import StandardScaler

# Предположим, 'Возраст' - числовой признак
scaler = StandardScaler()
df['Возраст_стандартизированный'] = scaler.fit_transform(df[['Возраст']])
```

### 3. Обработка категориальных признаков:

#### Пример: Преобразование категориальных признаков с использованием One-Hot Encoding

```python
df = pd.get_dummies(df, columns=['Пол'])
```

### 4. Преобразование временных признаков:

#### Пример: Извлечение года из столбца с датой

```python
df['Год'] = pd.to_datetime(df['Дата']).dt.year
```

### 5. Генерация полиномиальных признаков:

#### Пример: Создание полиномиальных признаков для числового столбца

```python
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2, include_bias=False)
poly_features = poly.fit_transform(df[['Признак']])
df_poly = pd.DataFrame(poly_features, columns=poly.get_feature_names(['Признак']))
```

### 6. Отбор признаков:

#### Пример: Использование важности признаков из модели

```python
from sklearn.ensemble import RandomForestClassifier

# Предположим, df - DataFrame с метками классов в столбце 'Целевая_переменная'
X = df.drop('Целевая_переменная', axis=1)
y = df['Целевая_переменная']

# Обучение модели
model = RandomForestClassifier()
model.fit(X, y)

# Важность признаков
feature_importance = model.feature_importances_
```

### 7. Инженерия признаков для текстовых данных:

#### Пример: Извлечение признаков из текста с использованием TF-IDF

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# Предположим, 'Текстовый_признак' - столбец с текстовой информацией
vectorizer = TfidfVectorizer()
text_features = vectorizer.fit_transform(df['Текстовый_признак'])
text_df = pd.DataFrame(text_features.toarray(), columns=vectorizer.get_feature_names_out())
```

Эти примеры демонстрируют базовые методы инженерии признаков в Python с использованием библиотеки pandas.
Для более сложных задач может потребоваться более продвинутая обработка данных и создание специализированных признаков.

Более продвинутая обработка данных включает в себя применение более сложных и интенсивных методов для подготовки и
преобразования данных с целью улучшения качества модели. Вот некоторые примеры продвинутых методов обработки данных:
1. Обработка временных рядов:

    Сглаживание данных: Использование методов скользящего среднего, экспоненциального сглаживания для уменьшения шума и
    выделения трендов.

    Функции автокорреляции (ACF) и частной автокорреляции (PACF): Используются для выявления зависимостей во
    временных рядах.

2. Работа с текстовыми данными:

    Векторизация слов: Использование Word Embeddings (например, Word2Vec, GloVe) для представления слов в числовой форме.

    TF-IDF с N-граммами: Расширение метода TF-IDF для учета последовательности из N слов.

    Анализ тональности: Определение эмоциональной окраски текста.

3. Обработка изображений:

    Аугментация данных: Генерация дополнительных обучающих изображений путем внесения случайных изменений
    (повороты, сдвиги, масштабирование и т.д.).

    Предварительное обучение с использованием сверточных нейронных сетей (CNN): Использование предварительно обученных
    моделей для извлечения признаков из изображений.

4. Обработка пропущенных значений:

    Импутация данных: Прогнозирование пропущенных значений с использованием моделей,
    таких как Random Forest или K-Nearest Neighbors.

    Методы булевой алгебры для обработки пропущенных значений в категориальных данных.

5. Масштабирование признаков:

    Нелинейное масштабирование: Применение нелинейных методов масштабирования,
     таких как Min-Max Scaling, Robust Scaling, Quantile Transformer.

6. Отбор признаков:

    Методы обратного отбора: Использование методов, таких как Recursive Feature Elimination (RFE),
    для пошагового удаления наименее важных признаков.

    Методы отбора признаков с использованием моделей: Оценка важности признаков на основе обученной модели.

7. Обработка несбалансированных классов:

    Методы взвешивания классов: Присвоение различных весов классам в зависимости от их доли в обучающей выборке.

    Генерация синтетических данных: Использование методов, таких как SMOTE (Synthetic Minority Over-sampling Technique),
     для генерации дополнительных примеров для классов с меньшим представлением.

Эти продвинутые методы обработки данных требуют более глубокого понимания задачи, особенностей данных и предметной области.
В их основе часто лежат техники машинного обучения и глубокого обучения.
